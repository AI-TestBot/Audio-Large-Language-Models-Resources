# Audio-Language-Models-Testing-Resources

## Audio

**AudioBench A Universal Benchmark for Audio Large Language Models.**<br>
*B Wang, X Zou, G Lin, S Sun, Z Liu, W Zhang, Z Liu, AT Aw, NF Chen.*<br>
arxiv:2406.16020, 2024.
[[ArXiv](https://arxiv.org/pdf/2406.16020)]
[[Github](https://github.com/AudioLLMs/AudioBench)]

**AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension.**<br>
*Q Yang, J Xu, W Liu, Y Chu, Z Jiang, X Zhou, Y Leng, Y Lv, Z Zhao, C Zhou, J Zhou.*<br>
arxiv:2402.07729, 2024.
[[ArXiv](https://arxiv.org/pdf/2402.07729)]
[[Github](https://github.com/OFA-Sys/AIR-Bench)]

**A Suite for Acoustic Language Model Evaluation.**<br>
*G Maimon, A Roth, Y Adi.*<br>
arxiv:2409.07437, 2024.
[[ArXiv](https://arxiv.org/pdf/2409.07437)]
[[Homepage](https://pages.cs.huji.ac.il/adiyoss-lab/salmon/)]

## Speech

**Fleurs: Few-shot learning evaluation of universal representations of speech.**<br>
*A Conneau, M Ma, S Khanuja, Y Zhang, V Axelrod, S Dalmia, J Riesa, C Rivera, A Bapna.*<br>
2022 IEEE Spoken Language Technology Workshop (SLT), 2023.
[[ArXiv](https://arxiv.org/pdf/2205.12446)]

**On the Evaluation of Speech Foundation Models for Spoken Language Understanding.**<br>
*S Arora, A Pasad, CM Chien, J Han, R Sharma, J Jung, H Dhamyal, W Chen, S Shon, H Lee, et al.*<br>
arXiv:2406.10083, 2024.
[[ArXiv](https://arxiv.org/pdf/2406.10083)]

**Dynamic-superb: Towards a dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech.**<br>
*C Huang, KH Lu, SH Wang, CY Hsiao, CY Kuan, H Wu, S Arora, KW Chang, J Shi, Y Peng, et al.*<br>
ICASSP, 2024.
[[ArXiv](https://arxiv.org/pdf/2309.09510)]
[[Gtihub](https://github.com/dynamic-superb/dynamic-superb)]

**A Large-Scale Evaluation of Speech Foundation Models.**<br>
*S Yang, HJ Chang, Z Huang, AT Liu, CI Lai, H Wu, J Shi, X Chang, HS Tsai, WC Huang, et al.*<br>
IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024.
[[ArXiv](https://arxiv.org/pdf/2404.09385)]

### Multilingual

**Covost 2 and massively multilingual speech-to-text translation.**<br>
*C Wang, A Wu, J Pino.*<br>
arXiv:2007.10310, 2020.
[[ArXiv](https://arxiv.org/pdf/2007.10310)]
[[Gtihub](https://github.com/facebookresearch/fairseq/tree/main/examples/speech_to_text)]

**Xtreme-s: Evaluating cross-lingual speech representations.**<br>
*A Conneau, A Bapna, Y Zhang, M Ma, et al.*<br>
arxiv, 2022.
[[ArXiv](https://arxiv.org/pdf/2203.10752)]

**Ml-superb: Multilingual speech universal performance benchmark.**<br>
*J Shi, D Berrebbi, W Chen, HL Chung, EP Hu, WP Huang, X Chang, SW Li, et al.*<br>
arxiv:2305.10615, 2023.
[[ArXiv](https://arxiv.org/pdf/2305.10615)]

### Dialogue

**SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond Words.**<br>
*J Ao, Y Wang, X Tian, D Chen, J Zhang, L Lu, Y Wang, H Li, Z Wu.*<br>
arxiv:2406.13340, 2024.
[[ArXiv](https://arxiv.org/pdf/2406.13340)]
[[Gtihub](https://github.com/amphionspace/SD-Eval)]

## Music

**MuChin A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music.**<br>
*Z Wang, S Li, T Zhang, Q Wang, P Yu, J Luo, Y Liu, M **, K Zhang.*<br>
arxiv:2402.09871, 2024.
[[ArXiv](https://arxiv.org/pdf/2402.09871)]
[[Homepage](https://github.com/CarlWangChina/MuChin/)]
